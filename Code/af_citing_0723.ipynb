{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get works that cite the AlphaFold paper\n",
    "url_with_cursor = \"https://api.openalex.org/works?filter=cites:W3177828909,type:article&cursor={}\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "cursor = '*'\n",
    "while cursor:\n",
    "    url = url_with_cursor.format(cursor)\n",
    "    response = requests.get(url).json()\n",
    "    data = pd.DataFrame(response['results'])\n",
    "    df = pd.concat([df, data])\n",
    "    cursor = response['meta']['next_cursor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df = df.drop(columns={'index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_index = df[df['is_authors_truncated']==True].index\n",
    "for i in truncated_index:\n",
    "    id = df.loc[i, 'id'].split('/')[-1]\n",
    "    url = f\"https://api.openalex.org/works/{id}\"\n",
    "    response = requests.get(url).json()\n",
    "    authorships = response['authorships']\n",
    "    df.at[i, 'authorships'] = authorships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_authors = []\n",
    "for i in range(len(df)):\n",
    "    for author in df.iloc[i]['authorships']:\n",
    "        af_authors.append(author['author']['id'])\n",
    "af_authors = list(set(af_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can filter using multiple IDs by using the '|' operator. (Up to 50 values)\n",
    "\n",
    "def build_author_works_url(id_list):\n",
    "    # specify endpoint\n",
    "    endpoint = 'works'\n",
    "\n",
    "    ids = '|'.join(id_list)\n",
    "\n",
    "    # build the 'filter' parameter\n",
    "    filters = (\n",
    "      f'author.id:{ids}',\n",
    "      'type:article', #excludes book-chapter, dissertation, book, dataset, paratext, other, reference-entry, report, peer-review, standard, editorial, erratum, grant, letter\n",
    "    )\n",
    "\n",
    "    # put the URL together\n",
    "    return f'https://api.openalex.org/{endpoint}?filter={\",\".join(filters)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publications(id_longlist, done_ids=None, start=0):\n",
    "    if done_ids is None:\n",
    "        done_ids = []\n",
    "    \n",
    "    id_longlist = list(set(id_longlist) - set(done_ids))\n",
    "    id_chunks = [id_longlist[i:i + 50] for i in range(0, len(id_longlist), 50)]\n",
    "    counter = start\n",
    "\n",
    "    for id_list in id_chunks:\n",
    "        data = pd.DataFrame()  # Initialize data for each chunk\n",
    "        url_with_cursor = build_author_works_url(id_list) + \"&cursor={}\"\n",
    "        cursor = '*'\n",
    "        \n",
    "        while cursor:\n",
    "            url = url_with_cursor.format(cursor)\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                response_json = response.json()\n",
    "                chunk_data = pd.DataFrame(response_json['results'])\n",
    "                data = pd.concat([data, chunk_data], ignore_index=True)\n",
    "                cursor = response_json['meta']['next_cursor']\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching data: {e}\")\n",
    "                break\n",
    "        \n",
    "        # data = data.drop_duplicates()  # Remove any duplicates\n",
    "        \n",
    "        counter += 1\n",
    "        print(f'Finished {counter * 50} out of {len(id_longlist)} authors')\n",
    "        print(f\"Size of data before saving: {data.memory_usage().sum() / 1e6} MB\")\n",
    "        \n",
    "        # Save the dataframe for this chunk\n",
    "        with open(f'af_authors_works_0724_interim_{counter}.pkl', 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "        \n",
    "        done_ids += id_list\n",
    "        \n",
    "        # Save done_ids after each chunk\n",
    "        with open('done_ids.pkl', 'wb') as file:\n",
    "            pickle.dump(done_ids, file)\n",
    "\n",
    "    return done_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_ids = get_publications(af_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one file to check\n",
    "with open('af_authors_works_0724_interim_1.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('truncated_works_short.pkl', 'rb') as file:\n",
    "    truncated_works = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_ids(authorships):\n",
    "    author_ids = []\n",
    "    for author in authorships:\n",
    "        author_ids.append(author['author']['id'])\n",
    "    return author_ids\n",
    "\n",
    "def extract_journal(location):\n",
    "    if isinstance(location, float):\n",
    "        if np.isnan(location): return pd.Series([np.nan,np.nan])\n",
    "    if location is None:\n",
    "        return pd.Series([np.nan,np.nan])\n",
    "    if location['source'] is None:\n",
    "        return pd.Series([np.nan,np.nan])\n",
    "    else:\n",
    "        id = location['source']['id']\n",
    "        journal = location['source']['display_name']\n",
    "    return pd.Series([id, journal])\n",
    "\n",
    "def extract_topic(topic):\n",
    "    if isinstance(topic, float):\n",
    "        if np.isnan(topic): return pd.Series([np.nan, np.nan, np.nan, np.nan])\n",
    "    if topic is None:\n",
    "        return pd.Series([np.nan, np.nan, np.nan, np.nan])\n",
    "    topic_name = topic['display_name']\n",
    "    subfield_name = topic['subfield']['display_name']\n",
    "    field_name = topic['field']['display_name']\n",
    "    domain_name = topic['domain']['display_name']\n",
    "    return pd.Series([topic_name, subfield_name, field_name, domain_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('truncated_works_ids.pkl', 'rb') as file:\n",
    "    truncated_works_ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = truncated_works_ids[0]\n",
    "id = i.split('/')[-1]\n",
    "url = f\"https://api.openalex.org/works/{id}\"\n",
    "response = requests.get(url).json()\n",
    "time.sleep(0.01)\n",
    "authorships = response['authorships']\n",
    "author_ids = extract_author_ids(authorships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['id', 'author_ids'])\n",
    "df = pd.concat([df, pd.DataFrame({'id': [id], 'author_ids': [author_ids]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['id', 'author_ids'])\n",
    "notdonelist = []\n",
    "for i in truncated_works_ids:\n",
    "    id = i.split('/')[-1]\n",
    "    url = f\"https://api.openalex.org/works/{id}\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        time.sleep(0.01)\n",
    "        authorships = response['authorships']\n",
    "        author_ids = extract_author_ids(authorships)\n",
    "        df = pd.concat([df, pd.DataFrame({'id': [id], 'author_ids': [author_ids]})], ignore_index=True)\n",
    "        print(f'{id} done')\n",
    "    except:\n",
    "        print(f'{id} skipped')\n",
    "        notdonelist.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in notdonelist:\n",
    "    id = i.split('/')[-1]\n",
    "    url = f\"https://api.openalex.org/works/{id}\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        time.sleep(0.01)\n",
    "        authorships = response['authorships']\n",
    "        author_ids = extract_author_ids(authorships)\n",
    "        df = pd.concat([df, pd.DataFrame({'id': [id], 'author_ids': [author_ids]})], ignore_index=True)\n",
    "        print(f'{id} done')\n",
    "    except:\n",
    "        print(f'{id} skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('truncated_authorships.pkl','wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated_works['fixed'] = 0\n",
    "count = 0\n",
    "# truncated_works_fixed = pd.DataFrame()\n",
    "while len(truncated_works)>0:\n",
    "    left = len(truncated_works)\n",
    "    for i in truncated_works.index:\n",
    "        id = truncated_works.loc[i, 'id'].split('/')[-1]\n",
    "        url = f\"https://api.openalex.org/works/{id}\"\n",
    "        try:\n",
    "            response = requests.get(url).json()\n",
    "            time.sleep(0.01)\n",
    "            authorships = response['authorships']\n",
    "            authors_count = len(authorships)\n",
    "            author_ids = extract_author_ids(authorships)\n",
    "            truncated_works.at[i, 'author_ids'] = author_ids\n",
    "            truncated_works.at[i, 'authors_count'] = authors_count\n",
    "            truncated_works.at[i, 'fixed'] = 1\n",
    "            print(f'{id} done ({i}/{left})')\n",
    "        except:\n",
    "            print(f'{id} skipped')\n",
    "    truncated_works_fixed = pd.concat([truncated_works_fixed, truncated_works[truncated_works['fixed']==1]])\n",
    "    truncated_works = truncated_works[truncated_works['fixed']==0]\n",
    "    print(f'cycle finished: {len(truncated_works)} left')\n",
    "    if len(truncated_works) == left:\n",
    "        count += 1\n",
    "        if count == 2:\n",
    "            with open('truncated_works_unfixed.pkl', 'wb') as file:\n",
    "                pickle.dump(truncated_works, file)\n",
    "            break\n",
    "            \n",
    "with open('truncated_works_fixed.pkl', 'wb') as file:\n",
    "    pickle.dump(truncated_works_fixed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = truncated_works.iloc[0]['id'].split('/')[-1]\n",
    "url = f\"https://api.openalex.org/works/{test_id}\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('af_authors_0725.pkl', 'rb') as file:\n",
    "    af_authors = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_works_fixed['has_af_authors'] = truncated_works_fixed['author_ids'].apply(lambda x: any([id in af_authors for id in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_authors(author_ids, af_authors_set, chunk_size=1000):\n",
    "    return np.array([any(author_id in af_authors_set for author_id in ids) for ids in author_ids])\n",
    "\n",
    "def process_dataframe(df, af_authors, chunk_size=1000):\n",
    "    # Convert af_authors to a set for faster lookup\n",
    "    af_authors_set = set(af_authors)\n",
    "    \n",
    "    # Convert 'author_ids' to lists if it's not already\n",
    "    if df['author_ids'].dtype != 'object':\n",
    "        df['author_ids'] = df['author_ids'].apply(lambda x: x if isinstance(x, list) else eval(x))\n",
    "    \n",
    "    # Initialize the result column\n",
    "    df['has_af_authors'] = False\n",
    "    \n",
    "    # Process the dataframe in chunks\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk_result = check_authors(chunk['author_ids'], af_authors_set)\n",
    "            df.loc[chunk.index, 'has_af_authors'] = chunk_result\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "# truncated_works_fixed = process_dataframe(truncated_works_fixed, af_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_works_fixed = process_dataframe(truncated_works_fixed, af_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_works_has_af_authors = truncated_works_fixed[truncated_works_fixed['has_af_authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_authors_set = set(af_authors)\n",
    "truncated_works_has_af_authors['af_authors'] = truncated_works_has_af_authors['author_ids'].apply(lambda x: [id for id in x if id in af_authors_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_works_has_af_authors['af_authors_count'] = truncated_works_has_af_authors['af_authors'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_authors_with_truncated_works = truncated_works_has_af_authors['af_authors'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(af_authors_with_truncated_works))\n",
    "af_authors_with_truncated_works = list(set(af_authors_with_truncated_works))\n",
    "print(len(af_authors_with_truncated_works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_truncated_works_dict = {}\n",
    "for author_id in af_authors_with_truncated_works:\n",
    "    af_truncated_works_dict[author_id] = truncated_works_has_af_authors[truncated_works_has_af_authors['af_authors'].apply(lambda x: author_id in x)]\n",
    "\n",
    "with open('af_truncated_works_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(af_truncated_works_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('journal_list.pkl', 'rb') as file:\n",
    "    journal_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can filter using multiple ORCIDs by using the '|' operator. (Up to 50 values)\n",
    "def build_url(id_list):\n",
    "    # specify endpoint\n",
    "    endpoint = 'sources'\n",
    "\n",
    "    ids = '|'.join(id_list)\n",
    "\n",
    "    # put the URL together\n",
    "    return f'https://api.openalex.org/{endpoint}?filter=openalex:{ids}&per-page=50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_info(id_longlist):\n",
    "    # split the list into chunks of 50\n",
    "    chunks = len(id_longlist)//50\n",
    "    count = 0\n",
    "    id_chunks = [id_longlist[i:i + 50] for i in range(0, len(id_longlist), 50)]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # get the URL\n",
    "    for id_list in id_chunks:\n",
    "        url = build_url(id_list)\n",
    "        response = requests.get(url).json()\n",
    "        data = pd.DataFrame(response['results'])\n",
    "        df = pd.concat([df, data])\n",
    "        count += 1\n",
    "        print(f'{count}/{chunks}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = get_journal_info(journal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('journal_df.pkl', 'wb') as file:\n",
    "    pickle.dump(journal_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df = journal_df[['id', 'issn_l', 'issn', 'display_name', 'type', 'is_oa', 'apc_usd', 'country_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[journal_df['issn_l'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie = pd.read_csv('scie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie_issns = scie['ISSN'].tolist() + scie['eISSN'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_df[journal_df['issn_l'].isin(scie_issns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie['issn'] = scie[['ISSN', 'eISSN']].apply(lambda x: x[0] if pd.notnull(x[0]) else x[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie['issn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esci = pd.read_csv('esci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esci['issn'] = esci[['ISSN', 'eISSN']].apply(lambda x: x[0] if pd.notnull(x[0]) else x[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issn_list = list(set(scie['issn'].tolist() + esci['issn'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save issn_list to text files with 600 issns each. ISSNs should be separated by semicolons.\n",
    "issn_chunks = [issn_list[i:i + 600] for i in range(0, len(issn_list), 600)]\n",
    "\n",
    "for i, chunk in enumerate(issn_chunks):\n",
    "    with open(f'issn_chunks/issn_chunk_{i}.txt', 'w') as file:\n",
    "        file.write(';'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_csv_reader(file_path, skip_rows=2, skip_footer=3):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip initial rows\n",
    "        for _ in range(skip_rows):\n",
    "            next(file)\n",
    "        \n",
    "        # Read the rest of the file\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        # Remove footer\n",
    "        lines = lines[:-skip_footer] if skip_footer else lines\n",
    "        \n",
    "        # Remove trailing comma from data rows (not from header)\n",
    "        header = lines[0].strip()+'\\n'\n",
    "        data_rows = [line.rstrip().rstrip(',')+'\\n' for line in lines[1:]]\n",
    "        \n",
    "        # Combine header and cleaned data rows\n",
    "        cleaned_csv = io.StringIO(header + ''.join(data_rows))\n",
    "        \n",
    "        # Read the cleaned CSV with pandas\n",
    "        df = pd.read_csv(cleaned_csv)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie_esci = pd.DataFrame()\n",
    "for i in range(1,31):\n",
    "    df = custom_csv_reader(f'jcr_chunks/KyuhunLee_JCR_JournalResults_08_2024-{i}.csv', skip_rows=2, skip_footer=2)\n",
    "    scie_esci = pd.concat([scie_esci, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_issn_list = scie_esci['ISSN'].tolist() + scie_esci['eISSN'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_downloaded_issn_list = list(set(issn_list) - set(downloaded_issn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scie_left = scie[(scie['issn'].isin(not_downloaded_issn_list)) | (scie['eISSN'].isin(not_downloaded_issn_list))]\n",
    "esci_left = esci[(esci['issn'].isin(not_downloaded_issn_list)) | (esci['eISSN'].isin(not_downloaded_issn_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_issn_list = list(set(scie_left['issn'].tolist() + scie_left['eISSN'].tolist() + esci_left['issn'].tolist() + esci_left['eISSN'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan value\n",
    "new_issn_list = [issn for issn in new_issn_list if pd.notnull(issn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new_issn_list to text files with 600 issns each. ISSNs should be separated by semicolons.\n",
    "new_issn_chunks = [new_issn_list[i:i + 600] for i in range(0, len(new_issn_list), 600)]\n",
    "\n",
    "for i, chunk in enumerate(new_issn_chunks):\n",
    "    with open(f'issn_chunks/new_issn_chunk_{i}.txt', 'w') as file:\n",
    "        file.write(';'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    df = custom_csv_reader(f'jcr_chunks/KyuhunLee_JCR_JournalResults_08_2024-{i}n.csv', skip_rows=2, skip_footer=2)\n",
    "    scie_esci = pd.concat([scie_esci, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jcr.pkl', 'wb') as file:\n",
    "    pickle.dump(scie_esci, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
