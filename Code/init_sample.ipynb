{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API keys\n",
    "api_springer = \"\" #full text for open-access articles in XML format (api.springernature.com/openaccess/jats/doi/[DOI]?api_key=[API])\n",
    "api_elsevier = \"\" #full text for any article I have access to in XML format (https://api.elsevier.com/content/article/doi/[DOI]?APIKey=[API]?view=FULL)\n",
    "api_wiley = \"\" #full text, but PDF download only.\n",
    "api_gpt = \"\"\n",
    "#BioC API: offers PMC OA and Author Manuscript Collection in XML format via PubMed ID or PMC ID (https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_[format]/[ID]/[encoding])\n",
    "api_semanticscholar = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can filter using multiple ORCIDs by using the '|' operator. (Up to 50 values)\n",
    "\n",
    "def build_author_works_url(id_list):\n",
    "    # specify endpoint\n",
    "    endpoint = 'works'\n",
    "\n",
    "    ids = '|'.join(id_list)\n",
    "\n",
    "    # build the 'filter' parameter\n",
    "    filters = (\n",
    "      f'author.id:{ids}',\n",
    "      'type:article', #excludes book-chapter, dissertation, book, dataset, paratext, other, reference-entry, report, peer-review, standard, editorial, erratum, grant, letter\n",
    "      'is_paratext:false' #excludes paratext\n",
    "    )\n",
    "\n",
    "    # put the URL together\n",
    "    return f'https://api.openalex.org/{endpoint}?filter={\",\".join(filters)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publications(id_longlist):\n",
    "    # split the list into chunks of 50\n",
    "    id_chunks = [id_longlist[i:i + 50] for i in range(0, len(id_longlist), 50)]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # get the URL\n",
    "    for id_list in id_chunks:\n",
    "        url_with_cursor = build_author_works_url(id_list) + \"&cursor={}\"\n",
    "\n",
    "        # initialize\n",
    "        cursor = '*'\n",
    "        while cursor:\n",
    "            url = url_with_cursor.format(cursor)\n",
    "            response = requests.get(url).json()\n",
    "            data = pd.DataFrame(response['results'])\n",
    "            df = pd.concat([df, data])\n",
    "            cursor = response['meta']['next_cursor']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get source ID for nature and science\n",
    "url = \"https://api.openalex.org/sources?filter=display_name.search:nature|science\"\n",
    "response = requests.get(url).json()\n",
    "for result in response['results']:\n",
    "    print(result['id'], result['display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nature_ID = response['results'][0]['id'] #https://openalex.org/S137773608\n",
    "# Science_ID = response['results'][1]['id'] #https://openalex.org/S3880285\n",
    "Nature_ID = \"https://openalex.org/S137773608\"\n",
    "Science_ID = \"https://openalex.org/S3880285\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get works published in Nature and Science\n",
    "def NS_works(year):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    filters = (\n",
    "        'type:article', #excludes book-chapter, dissertation, book, dataset, paratext, other, reference-entry, report, peer-review, standard, editorial, erratum, grant, letter\n",
    "        'is_paratext:false', #excludes paratext\n",
    "        f'primary_location.source.id:{Nature_ID}|{Science_ID}',\n",
    "        f'publication_year:{year}'\n",
    "    )\n",
    "\n",
    "    url_with_cursor = f'https://api.openalex.org/works?filter={\",\".join(filters)}' + '&cursor={}'\n",
    "    # initialize\n",
    "    cursor = '*'\n",
    "    while cursor:\n",
    "        url = url_with_cursor.format(cursor)\n",
    "        response = requests.get(url).json()\n",
    "        data = pd.DataFrame(response['results'])\n",
    "        df = pd.concat([df, data])\n",
    "        cursor = response['meta']['next_cursor']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2018 = NS_works(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2020 = NS_works(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2020.to_parquet('ns_2020.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2018.to_parquet('ns_2018.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2018_parquet = pd.read_parquet('ns_2018.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2018_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_2018['authorships']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids_2 = []\n",
    "for authorships in ns_2018['authorships']:\n",
    "    author_list = [d['author']['id'] for d in authorships]\n",
    "    author_ids_2.append(author_list)\n",
    "\n",
    "author_ids_2 = list(set(chain.from_iterable(author_ids_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = []\n",
    "for authorships in ns_2018_parquet['authorships']:\n",
    "    author_list = [d['author']['id'] for d in authorships]\n",
    "    author_ids.append(author_list)\n",
    "\n",
    "author_ids = list(set(chain.from_iterable(author_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids_2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save author_ids\n",
    "with open('author_ids.txt', 'w') as f:\n",
    "    for item in author_ids:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load author_ids disregarding the newline character\n",
    "with open('author_ids.txt', 'r') as f:\n",
    "    author_ids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_author_ids = []\n",
    "for authorships in ns_2018['authorships']:\n",
    "    author_list = [d['author']['id'] for d in authorships if d['is_corresponding'] or d['author_position']=='last']\n",
    "    corr_author_ids.append(author_list)\n",
    "\n",
    "corr_author_ids = list(set(chain.from_iterable(corr_author_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_author_ids = []\n",
    "for authorships in ns_2018_parquet['authorships']:\n",
    "    author_list = [d['author']['id'] for d in authorships if d['is_corresponding'] or d['author_position']=='last']\n",
    "    corr_author_ids.append(author_list)\n",
    "\n",
    "corr_author_ids = list(set(chain.from_iterable(corr_author_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corr_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, N):\n",
    "    return [input_list[i:i+N] for i in range(0, len(input_list),N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_chunks = chunk_list(author_ids,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_chunks[511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save author_chunks\n",
    "with open('author_chunks.txt', 'w') as f:\n",
    "    for item in author_chunks:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load author_chunks disregarding the newline character\n",
    "with open('author_chunks.txt', 'r') as f:\n",
    "    author_chunks = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_chunks = [eval(x) for x in author_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_ids(authorships):\n",
    "    author_ids = []\n",
    "    for authorship in authorships:\n",
    "        author_ids.append(authorship['author']['id'])\n",
    "    return author_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_author_id_list(chunk):\n",
    "    chunk['author_ids'] = chunk['authorships'].apply(extract_author_ids)\n",
    "    return chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50]:\n",
    "    try:\n",
    "        df = get_publications(author_chunks[i])\n",
    "        df = add_author_id_list(df)\n",
    "        df.to_parquet(f'author_publications_{i}.parquet', engine='fastparquet')\n",
    "        # write progress in txt\n",
    "        with open('ns2018_progress.txt', 'a') as f:\n",
    "            f.write(f\"chunk {i} done\\n\")\n",
    "    except:\n",
    "        with open('ns2018_errorlist.txt', 'a') as f:\n",
    "            f.write(f\"chunk {i} failed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(build_author_works_url(author_chunks[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_parquet('/Volumes/Samsung_T5/OpenAlex_Works/author_publications_7.parquet 2')\n",
    "df2 = pd.read_parquet('/Volumes/Samsung_T5/OpenAlex_Works/author_publications_7.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(author_chunks)):\n",
    "    chunk = author_chunks[i]\n",
    "    df = pd.read_parquet(f'/Volumes/Samsung_T5/OpenAlex_Works/author_publications_{i}.parquet')\n",
    "    df.author_ids = df.author_ids.apply(lambda x: eval(x))\n",
    "    for author in chunk:\n",
    "        author_name = author.split('/')[-1]\n",
    "        author_df = df[df.author_ids.apply(lambda x: author in x)]\n",
    "        author_df.to_parquet(f'/Volumes/Samsung_T5/OpenAlex_Works/author_publications_{author_name}.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_meta = pd.read_csv('metadata.csv', usecols=['corpusid','externalids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_meta['externalids'] = s2orc_meta.externalids.apply(lambda x: eval(x) if pd.notna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2orc_mag = s2orc_meta.externalids.apply(lambda x:)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
